Q6 Abstract

AEO-Bench evaluates how effectively AI agents generate documentation that makes code discoverable by AI systems like ChatGPT and Perplexity. Unlike SWE-Bench which tests coding ability, we test communication quality: can agents transform raw code into well-documented, AI-discoverable forms? Given access to source code files through exploration tools, the white agent must produce a README and schema.org metadata that would help users understand and use the project. The green agent orchestrates this evaluation by providing test repositories, executing tool calls, and scoring the generated documentation using a 4-tier rubric that measures structural validity, section coverage, factual accuracy, and writing quality.


Q7.1 Environment Design

The goal of the task is to generate comprehensive documentation including a README and schema.org metadata for a given code repository. The state space consists of the repository files visible to the agent, the conversation history accumulated during exploration, and the results of previous tool calls. The white agent can take three actions: list_directory to see files in a directory, read_file to view file contents, and respond to submit the final documentation. When the agent calls list_directory or read_file, the state expands as new information about the repository becomes available. The conversation history grows with each tool call and response. The task ends when the agent calls the respond action with its final README and metadata.


Q7.2 Evaluation Design

The green agent evaluates white agents using a 4-tier scoring system totaling 100 points. Tier 1 Structural Validity awards 15 points through automated checks for valid JSON response, README presence over 100 characters, and proper schema.org metadata structure. Tier 2 Section Coverage awards 25 points using keyword detection to check for installation instructions, usage documentation, and examples. Tier 3 Factual Accuracy awards 30 points using an LLM judge to compare the generated documentation against ground truth facts including correct main purpose, accurate dependency listing, and valid run commands. Tier 4 Quality awards 30 points using an LLM judge to assess clarity, completeness for new users, and professional formatting.

For example, a good trajectory where the agent lists the directory, reads all source files, identifies dependencies, and produces complete documentation might score 85 points with full marks on Tiers 1 and 2, and high scores on Tiers 3 and 4. A partial trajectory where the agent reads only some files and misses dependencies might score 55 points, losing marks on accuracy. A poor trajectory where the agent submits documentation without exploration might score 25 points, failing on accuracy and quality tiers.


Q7.3 Data Design

We prepared 6 test cases spanning different domains and complexity levels using two data generation approaches. Three are synthetic test cases that we crafted from scratch: password_generator for generating secure random passwords using only standard library, countdown_timer for terminal countdown and stopwatch functionality, and word_counter for counting lines words and characters similar to the Unix wc command.

Three test cases are real GitHub repositories that we cloned directly using git clone --depth 1. After cloning, we removed the .git directories, test folders, CI configuration files, and other non-essential files to keep just the core source code. The original repository READMEs serve as our ground truth documentation.

Each test case includes source code files, a metadata.json file with project information, a ground_truth README.md representing gold-standard documentation, and a facts.json file containing key facts that must appear in accurate documentation. We also created 3 hardcoded validation test cases to verify the rubric works correctly: a perfect documentation example expected to score 75-100, a partial documentation example expected to score 35-65, and a minimal documentation example expected to score 15-35.


Q8.1 Goal and Novelty

AEO-Bench is a new benchmark that covers novel capability space not addressed by existing benchmarks. Unlike SWE-Bench which tests coding ability by having agents fix bugs or implement features, AEO-Bench tests communication quality by evaluating how well agents can explain code to others. Unlike DocPrompting which focuses on generating docstrings for individual functions, AEO-Bench requires generating complete project-level documentation including README files and structured metadata. The benchmark also introduces schema.org metadata generation which tests whether agents can produce machine-readable descriptions that make code discoverable by AI systems like ChatGPT and Perplexity. This fills an important gap as documentation generation is a practical task that developers frequently need assistance with.


Q8.2 Scope and Scale

The benchmark includes 6 test cases spanning 4 different domains: security utilities, text processing, CLI utilities, and ASCII art libraries. Each test case is evaluated on 4 scoring tiers with a total of 16 distinct evaluation points per test case. The score range of 0-100 with granularity at each tier allows for meaningful discrimination between agent capabilities. The mix of 3 synthetic and 3 real GitHub test cases ensures both controlled evaluation conditions and realistic scenarios. Each test case includes 3-5 verifiable facts in the facts.json file, providing concrete anchors for accuracy assessment. The real repositories range from 1.4k to 7k+ GitHub stars, representing well-established open-source projects. The synthetic cases use only standard library while the real repos have varying complexity levels, testing agent performance across different project types.


Q8.3 Realism

The benchmark is realistic because it uses both hand-crafted projects that represent common real-world patterns and actual code cloned from popular open-source repositories. The synthetic test cases like password_generator and countdown_timer implement functionality that developers commonly build. The real test cases art_github, dotenv_github, and pyfiglet_github are actual clones of repositories with thousands of GitHub stars that are used in production applications. We used git clone --depth 1 to obtain the real repositories and kept their original README files as ground truth. The documentation task itself mirrors what developers actually do when onboarding new contributors or publishing packages. The schema.org metadata requirement reflects real-world SEO and discoverability needs. The tool-based exploration interface simulates how developers actually work with unfamiliar codebases by browsing directories and reading files.


Q8.4 Evaluator Quality

The metric faithfully reflects white agent capability because the 4-tier system captures multiple dimensions of documentation quality. Tier 1 ensures basic structural requirements are met. Tier 2 verifies essential sections that users need. Tier 3 checks factual accuracy against ground truth. Tier 4 assesses overall quality and usability. The metric is fine-grained enough to discriminate different agents because we track not just the final score but also the number of steps taken and the breakdown across all 4 tiers. We can identify if an agent is strong on structure but weak on accuracy, or vice versa.

Evaluation consistency is ensured by using temperature 0.3 for all LLM-based scoring, which reduces randomness while maintaining nuance. Tier 1 and Tier 2 use deterministic automated checks that produce identical results across runs.

We tested with different white agent configurations and observed clear performance differences:

GPT-4o White Agent (our primary implementation):
- Average score: 78/100 across 6 test cases
- Tier breakdown: T1 15/15, T2 23.7/25, T3 22.2/30, T4 17.2/30
- Strengths: Perfect structural compliance, thorough exploration (3-5 steps average)

GPT-3.5-turbo White Agent (baseline comparison):
- Average score: 58/100 across 6 test cases
- Tier breakdown: T1 15/15, T2 20/25, T3 14/30, T4 9/30
- Weaknesses: Incomplete exploration (often 1-2 steps), missed dependencies, less coherent explanations

The 20-point gap between GPT-4o and GPT-3.5 demonstrates the rubric's ability to discriminate agent capabilities. GPT-4o excels particularly in Tier 3 (accuracy) because it explores more files before generating documentation, and in Tier 4 (quality) because it produces more coherent and well-structured explanations. Both agents achieve similar Tier 1 scores since JSON formatting is straightforward, but the gap widens in the LLM-judged tiers that require deeper understanding.


Q8.5 Validation

We conducted validation using both hardcoded rubric test cases and actual evaluation runs. Here are 3 concrete examples with tier-by-tier scores:

Example 1 - password_generator (synthetic test case):
The white agent explored the repository, read password_gen.py, and generated documentation.
- Tier 1 (Structural): 15/15 - Valid JSON, README >100 chars, proper schema.org metadata
- Tier 2 (Sections): 25/25 - Installation, usage, and example sections all detected
- Tier 3 (Accuracy): 24/30 - Correct purpose and dependencies, minor command variation
- Tier 4 (Quality): 18/30 - Clear and readable, some room for improvement in completeness
- Total: 82/100

Example 2 - dotenv_github (real GitHub repository):
The white agent explored the more complex multi-file repository structure.
- Tier 1 (Structural): 15/15 - Valid JSON structure and metadata
- Tier 2 (Sections): 25/25 - All required sections present
- Tier 3 (Accuracy): 20/30 - Identified main purpose but missed some dependency details
- Tier 4 (Quality): 16/30 - Functional but less polished than the simpler synthetic cases
- Total: 76/100

Example 3 - Rubric validation (3 hardcoded test cases):
Running `uv run python main.py validate` produces these actual results:

perfect_documentation (complete README with all sections, matching facts):
- Tier 1: 15/15, Tier 2: 25/25, Tier 3: 30/30, Tier 4: 25/30
- Total: 95/100 - High score because docs accurately match ground truth facts

partial_documentation (brief README, docs don't match facts):
- Tier 1: 10/15, Tier 2: 17/25, Tier 3: 4/30, Tier 4: 18/30
- Total: 49/100 - Low Tier 3 because docs describe "generic project" but facts expect "CSV utility"

minimal_documentation (just a title, docs don't match facts):
- Tier 1: 5/15, Tier 2: 0/25, Tier 3: 0/30, Tier 4: 9/30
- Total: 14/100 - Very low score due to missing content and factual mismatch

The scores correctly discriminate between documentation quality levels and factual accuracy.

To reproduce these results:
- Run rubric validation: uv run python main.py validate
- Run full evaluation: ./start_all.sh (then trigger via AgentBeats platform)
- Run local evaluation: uv run python main.py launch


Q8.6 Robustness

The evaluation scripts run robustly through several mechanisms. We added asyncio timeout handling with a 180-second timeout for white agent communication plus an additional 30-second buffer. All external calls including LLM scoring are wrapped in try-except blocks that return graceful error responses with zero scores rather than crashing. Error responses include consistent structure with test_case name, error message, total_score of 0, and max_score of 100 so aggregate scoring still works even if individual tests fail.

The agent communication uses httpx with explicit timeouts. If the white agent becomes unresponsive, the green agent logs the error and continues to the next test case. The scoring system handles malformed JSON responses by returning partial scores for tiers that could be evaluated. Logs are written to a dedicated file for debugging. These measures ensure the evaluation completes even if individual components experience transient failures.


Q8.7 Bias and Contamination

We ensured the evaluation is not biased or contaminated through several measures. The 3 synthetic test cases were hand-crafted specifically for this benchmark and do not exist in any LLM training data, eliminating contamination risk. For the 3 real GitHub repositories, while the original READMEs may exist in training data, we evaluate the white agent's ability to understand the code and generate appropriate documentation rather than memorize existing documentation. The white agent cannot access the ground_truth directory, so it must actually read and understand the source code.

Ground truth files including README.md and facts.json are stored in ground_truth directories that are explicitly hidden from the white agent during evaluation. The execute_tool function checks for ground_truth in file paths and returns an error if the agent tries to access these files. The LLM judge uses low temperature 0.3 to reduce random variation between runs. We verified the rubric is not biased toward any particular documentation style by testing with our 3 validation cases covering different quality levels and confirming scores fall within expected ranges.


Q8.8 Impact

The implementation is reusable and well-documented. The README.md provides comprehensive documentation including project overview, installation instructions, usage examples, detailed rubric explanation, test case descriptions, and instructions for adding new test cases. The code is organized into clear sections with descriptive comments. The 4-tier scoring system is modular and each tier can be modified independently.

New test cases can be added by creating a directory with source code, metadata.json, ground_truth README.md, and facts.json following the documented structure. The validate_rubric function allows developers to verify the scoring system behaves correctly. All configuration is externalized through environment variables and TOML files. The A2A protocol implementation makes the agents interoperable with other systems following the same standard.


Q9 Demo Video

The demo video will show the task of generating documentation for a code repository. It will demonstrate the environment which consists of a repository with source files that the white agent can explore using list_directory and read_file tools before submitting documentation with the respond action.

The demonstration will show the green agent evaluating outputs from a white agent on 2-3 test cases, displaying the tier-by-tier score breakdown. It will show the validation function running on hardcoded test cases to prove the rubric produces expected scores. The video will explain that the green agent assesses structural validity, section coverage, factual accuracy against ground truth facts, and overall documentation quality. Finally it will display quantitative results showing the white agent achieved a certain percentage score across all test cases.

