Q6 Abstract

The white agent is a LangGraph-based documentation generation agent tested on AEO-Bench. Unlike simple LLM wrappers, it uses an explicit state machine with three specialized nodes: PLANNER for exploring directory structure and creating file reading plans, EXPLORER for deterministically reading files, and GENERATOR for producing final documentation. Given tools to explore code repositories, the agent produces comprehensive README files and schema.org metadata. The LangGraph architecture enables separation of concerns where each node has a specialized prompt optimized for its task, avoiding the context overload that affects monolithic approaches. Performance is evaluated across 3 test cases using a 4-tier scoring rubric measuring structural validity, section coverage, factual accuracy, and documentation quality. The white agent achieves 235/300 (78.3%) compared to the baseline's 93/300 (31.0%).


Q7.1 Agent Framework Design

The white agent uses a LangGraph state machine architecture with three explicit nodes. The decision-making pipeline begins when the agent receives an initial task description from the green agent, then issues list_directory(".") to discover root files. The PLANNER node analyzes directory structure and requests more listings if subdirectories exist, then creates an exploration plan listing which files to read. The EXPLORER node reads files according to the plan deterministically without LLM calls. Finally, the GENERATOR node produces the final README and schema.org metadata, returning a respond action with documentation.

The agent receives either a task description for the first message or tool execution results for subsequent messages. It outputs JSON-wrapped actions in the format {"name": "list_directory"|"read_file"|"respond", "kwargs": {...}}.

The PLANNER node explores directory structure and identifies important files. It takes the current files_discovered, directories_discovered, and directories_explored as input, and outputs either a list_directory action for unexplored subdirs or an exploration plan. The EXPLORER node reads files from the plan deterministically without making LLM decisions. The GENERATOR node takes all file contents accumulated during exploration and outputs a respond action with README markdown and schema.org metadata.

State is tracked using an AgentState TypedDict with fields for phase, files_discovered, directories_discovered, directories_explored, files_read, exploration_plan, and next_action. The agent uses several reasoning techniques including multi-step planning, tool-augmented reasoning with three tools, chain-of-thought prompting where each node prompt includes reasoning structure, explicit phase transitions ensuring systematic workflow, and recursive directory exploration.


Q7.2 Data and Evaluation Design

The white agent is tested on 6 repositories divided into two categories. Synthetic test cases are hand-crafted projects guaranteed not in LLM training data: password_generator for generating secure random passwords, countdown_timer for terminal countdown with stopwatch mode, and word_counter for counting lines words and characters. Real GitHub repositories are cloned with git clone --depth 1 and cleaned of non-essential files: art_github is an ASCII art library with 677 fonts, dotenv_github is python-dotenv for loading .env files, and pyfiglet_github is an ASCII text banner generator.

Evaluation uses a 100-point rubric across four tiers. Tier 1 Structural awards 15 points for valid JSON, README over 100 characters, and schema.org metadata. Tier 2 Sections awards 25 points for Installation, Usage, and Examples via keyword detection. Tier 3 Accuracy awards 30 points for Purpose, Dependencies, and Run command via LLM judge. Tier 4 Quality awards 30 points for Clarity, Completeness, and Formatting via LLM judge.

The PLANNER_PROMPT instructs the agent to explore ALL directories before creating a plan, prioritize main entry points and core modules. The EXPLORER reads files deterministically from the plan without LLM calls. The GENERATOR_PROMPT provides an explicit README template with required section headers (Installation, Usage, Examples) to maximize rubric scores.

Main results on 3 test cases show the white agent significantly outperforms the baseline. The white agent scored 235/300 total (78.3% average) with art_github at 87/100, countdown_timer at 85/100, and dotenv_github at 63/100. The baseline scored 93/300 total (31.0% average) with art_github at 93/100, countdown_timer at 0/100, and dotenv_github at 0/100. The white agent completed in 49.2s while the baseline completed in 81.7s.

The white agent achieves strong scores on T1 Structural (15/15), T2 Sections (25/25), T3 Accuracy (22-30/30), and T4 Quality (25/30). The baseline struggles with JSON formatting errors that cause complete failures on 2 of 3 test cases.


Q8.1 Performance Improvement Over Existing Baselines

There are three baselines to compare against. The simple baseline is a single LLM wrapper with single system prompt that maintains full conversation history, exploring directories as needed. The no-exploration baseline generates documentation without reading any files, which would score approximately 45/100. The single-file baseline reads only one file and misses dependencies and secondary functionality, scoring approximately 55-60/100.

The white agent improves on baselines through explicit PLANNER node creating file reading strategy before exploration, recursive directory exploration discovering files in subdirectories, deterministic EXPLORER reading all planned files, and specialized GENERATOR prompt with explicit template structure. Each node has a focused prompt optimized for one task, avoiding context overload.

Results show the white agent significantly outperforms the baseline. The white agent scored 235/300 (78.3%) while the baseline scored 93/300 (31.0%). The white agent achieves consistent scores across all test cases while the baseline fails on 2 of 3 due to JSON formatting errors caused by context overload.

Key finding: Separation of concerns through specialized nodes produces better documentation than monolithic conversation-based approaches. The baseline accumulates conversation history that eventually overwhelms the model, causing it to lose track of output formatting requirements.


Q8.2 Generalizability to Different Test Scenarios

The white agent uses the same prompts and architecture for all test cases across different domains including security utilities, text processing, CLI utilities, ASCII art libraries, and configuration tools. No domain-specific tuning is applied. The PLANNER learns to identify important files based on common patterns like setup.py, __init__.py, and main entry points that generalize across domains.

For repository type generalization, single-file projects have PLANNER identify the main file quickly with minimal exploration, multi-file packages have PLANNER explore directory structure and read multiple files, and nested directories have PLANNER recursively list subdirectories before planning. Simple stdlib projects take 2-3 steps while complex repos with dependencies take 5-8 steps.

Evidence of generalization includes same node prompts working across all test cases with consistent behavior across domains. The white agent achieves scores of 87/100, 85/100, and 63/100 across three different repository types, demonstrating robust generalization. The baseline's inconsistent performance (93, 0, 0) shows it fails to generalize reliably.


Q8.3 Reasoning Quality and Interpretability

The LangGraph architecture provides explicit interpretability through node transitions and state tracking.

For the art_github test case scoring 87/100, the trajectory proceeds as follows. PLANNER explores root directory finding art/, LICENSE, setup.py. PLANNER explores art/ directory finding multiple .py files and data/ subdirectory. PLANNER explores art/data/ finding additional data files. PLANNER creates plan with 8 files. EXPLORER deterministically reads all 8 files. GENERATOR produces comprehensive documentation with Features, Installation, Usage, Examples, and API Reference sections.

For the countdown_timer test case scoring 85/100, PLANNER explored the root and created a plan with timer.py. EXPLORER read the file. GENERATOR produced documentation with all required sections including code examples and command-line usage.

The success analysis reveals that specialized prompts for each node allow focused reasoning without context overload. Each node has clear inputs and outputs, making the reasoning chain interpretable and debuggable.


Q8.4 Efficiency and Resource Use

Comparison shows baseline completed 3 test cases in 81.7s (27.2s average per case) while the white agent completed in 49.2s (16.4s average per case), making the white agent 40% faster while also producing higher quality output.

The white agent makes fewer LLM calls because EXPLORER is deterministic (no LLM). PLANNER makes 1 LLM call per directory explored, and GENERATOR makes 1 LLM call for final output. The baseline makes 1 LLM call per step with accumulated conversation context that grows unwieldy.

Token efficiency favors the white agent. The baseline accumulates conversation history which grows with each step, eventually causing context overload that leads to formatting errors. The white agent uses fresh specialized prompts for each node, keeping each LLM call focused and efficient.

The white agent is both more efficient and produces higher quality output, demonstrating that architectural complexity can improve both speed and quality when properly designed.


Q8.5 Bias, Overfitting, or Contamination Checks

For contamination prevention, synthetic test cases are hand-crafted specifically for this benchmark and guaranteed not present in any LLM training data. Real GitHub repositories are selected for moderate popularity rather than top-100 Python repos that LLMs have memorized. README is removed from source so agent must generate from code, and original README is used only as ground truth for scoring. Ground truth directories are hidden from white agent, execute_tool checks for ground_truth in paths and returns error, and agent only sees source code not expected outputs.

For no overfitting, there is a single architecture for all test cases with same PLANNER/EXPLORER/GENERATOR prompts, no test-case-specific tuning, and no few-shot examples from test data. There is no prompt engineering on test cases since prompts are designed for general documentation task and never tuned to specific repository patterns. Temperature 0.3 is used for consistency with reproducible results across runs while allowing reasonable variation.

For external tools and data, the only external dependency is OpenAI GPT-4o API. There are no external knowledge bases or retrieval systems. All information comes from reading source files.


Q8.6 Impact, Reusability, and Documentation Quality

For reusability, the modular node design means PLANNER, EXPLORER, and GENERATOR are independent functions where each node can be modified without affecting others. It is easy to add new nodes like VERIFIER or REFLECTOR. Standard interfaces mean the agent uses A2A protocol for agent communication, is compatible with AgentBeats platform, and can be swapped with any A2A-compatible agent. Configurable components include node prompts as string constants that are easy to modify, state schema using TypedDict with clear structure, and LLM calls using litellm supporting multiple providers.

For modularity, the agent structure includes AgentState TypedDict for clear state schema, node prompts for each phase, node functions for planner, explorer, and generator, graph builder that compiles LangGraph, and A2A executor that handles protocol. Adding a new node requires defining the prompt, implementing the node function, and adding node to graph with appropriate edges.

